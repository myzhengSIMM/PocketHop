{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@Time:     Created on 2021/01/11 14:29\n",
    "@author:   Tianbiao Yang & Mingyue Zheng\n",
    "@Email:    Tianbiao_Yang@163.com\n",
    "@Filename: training_graphbsm.py\n",
    "@Software: Spyder & Python\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys, os, random, warnings\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from random import shuffle\n",
    "import torch\n",
    "import timeit\n",
    "import torch.nn as nn\n",
    "from models.gat import GATNet\n",
    "from models.gat_gcn import GAT_GCN\n",
    "from models.gcn import GCNNet\n",
    "from models.ginconv import GINConvNet\n",
    "from models.transformer_gat import TRANSFORMER_GATNet\n",
    "from models.transformer_gcn import TRANSFORMER_GCNNet\n",
    "from Radam import *\n",
    "from utils import *\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, precision_recall_curve, auc\n",
    "import torch.nn.functional as F\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SetSeed(seed):\n",
    "    os.environ['PYTHONHASHSEED'] =str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    np.random.seed(seed)  # Numpy module.\n",
    "    random.seed(seed)  # Python random module.\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "def _init_fn(worker_id):\n",
    "    np.random.seed(int(1234))\n",
    "    \n",
    "SEED = 1234;\n",
    "SetSeed(SEED)\n",
    "  \n",
    "def save_AUCs(AUCs, filename):\n",
    "    with open(filename, 'a') as f:\n",
    "        f.write('\\t'.join(map(str, AUCs)) + '\\n')\n",
    "\n",
    "def LoadDataSet(SetName,dpath,BATCH_SIZE):\n",
    "    \n",
    "    processed_data_file_valid = dpath + 'Datasets/processed/' + SetName + '.pt'\n",
    "    processed_data_file_valid_t = dpath + 'Datasets/processed/' + SetName + '_t.pt'\n",
    "    valid_data = TestbedDataset(root=dpath + 'Datasets/', dataset= SetName)\n",
    "    valid_data_t = TestbedDataset(root=dpath + 'Datasets/', dataset=SetName + '_t')\n",
    "    # make data PyTorch mini-batch processing ready\n",
    "    valid_loader = DataLoader(valid_data, batch_size = BATCH_SIZE, shuffle=False, num_workers=0, worker_init_fn=_init_fn)\n",
    "    valid_loader_t = DataLoader(valid_data_t, batch_size = BATCH_SIZE, shuffle=False, num_workers=0, worker_init_fn=_init_fn)\n",
    "    VERTEXset = defaultdict(list)\n",
    "    for index,data in enumerate(valid_loader):\n",
    "        VERTEXset['PDB_A'].append(data)\n",
    "    for index,data_t in enumerate(valid_loader_t):\n",
    "        VERTEXset['PDB_B'].append(data_t)\n",
    "    \n",
    "    return VERTEXset['PDB_A'],VERTEXset['PDB_B']\n",
    "\n",
    "\n",
    "def train(model, device, train_loader,Loss, optimizer, epoch):\n",
    "    model.train()\n",
    "    value,value_t = train_loader[0],train_loader[1]\n",
    "    total_loss = 0\n",
    "    for n in range(0,len(value)):\n",
    "        data = (value[n].to(device),value_t[n].to(device))\n",
    "        # print(torch.tensor(data.y,dtype=torch.int64).view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = Loss(output, torch.as_tensor(data[0].y,dtype=torch.int64).view(-1).to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss/len(value)\n",
    "\n",
    "\n",
    "def predicting(model, device, Loss,loader):\n",
    "    model.eval()\n",
    "    # total_preds = torch.Tensor()\n",
    "    # total_labels = torch.Tensor()\n",
    "    T, Y, S = [], [], []\n",
    "    with torch.no_grad():\n",
    "        value,value_t = loader[0],loader[1]\n",
    "        total_loss = 0\n",
    "        for n in range(0,len(value)):\n",
    "            data = (value[n].to(device),value_t[n].to(device))\n",
    "            output = model(data)\n",
    "            loss = Loss(output, torch.as_tensor(data[0].y,dtype=torch.int64).view(-1).to(device))\n",
    "            total_loss += loss.item()\n",
    "            correct_labels = torch.as_tensor(data[0].y,dtype=torch.int64).view(-1)\n",
    "            correct_labels = correct_labels.to('cpu').data.numpy()\n",
    "            ys = F.softmax(output, 1).to('cpu').data.numpy()\n",
    "            predicted_labels = np.argmax(ys, axis=1)\n",
    "            predicted_scores = ys[:, 1]\n",
    "            T.extend(correct_labels)\n",
    "            Y.extend(predicted_labels)\n",
    "            S.extend(predicted_scores)\n",
    "            #print(T)\n",
    "            #print(S)\n",
    "        try:\n",
    "            AUC = roc_auc_score(T, S)\n",
    "            tpr, fpr, _ = precision_recall_curve(T, S)\n",
    "            PRC = auc(fpr, tpr)\n",
    "        except:\n",
    "            AUC,PRC = 0,0\n",
    "        precision = precision_score(T, Y)\n",
    "        recall = recall_score(T, Y)\n",
    "        # total_preds = torch.cat((total_preds, output.cpu()), 0)\n",
    "        # total_labels = torch.cat((total_labels, data.y.view(-1, 1).cpu()), 0)\n",
    "        \n",
    "    return AUC,PRC,total_loss/len(value),recall,precision\n",
    "\n",
    "\n",
    "def training_gat(hypers):\n",
    "    ## Hyper-parameter\n",
    "    modeling,BATCH_SIZE,LR,WEIGHT_DECAY,NUM_EPOCHS,DROPOUT,HID_DIM,SEED,HEADER,CUDA_NAME = hypers\n",
    "    \n",
    "    ## Loader dataset\n",
    "    datasets = ['BSMset_DelVerBare_train','BSMset_DelVerBare_valid','BSMset_DelVerBare_test','Vertex','Barelier']\n",
    "    dataset_loaders = list()\n",
    "    for dataset in datasets:\n",
    "        dpath = '/home/tbyang/Desktop/GraphBSM3/data/'\n",
    "        dataset_loaders.append(LoadDataSet(dataset,dpath,BATCH_SIZE))\n",
    "    train_loader, valid_loader, test_loader = dataset_loaders[0],dataset_loaders[1],dataset_loaders[2]\n",
    "    vertex_loader, barelier_loader = dataset_loaders[3],dataset_loaders[4]\n",
    "\n",
    "    ## Select the modeling\n",
    "    # modeling = GATNet\n",
    "    max_auc, best_epoch_value = 0,0\n",
    "    model_st = modeling.__name__\n",
    "    cuda_name = CUDA_NAME\n",
    "    print('running on ', model_st + '_' + datasets[0])\n",
    "    Hyper = [model_st,NUM_EPOCHS,BATCH_SIZE,str(LR).split('.')[-1],str(DROPOUT).split('.')[-1],HEADER,HID_DIM] \n",
    "    \n",
    "    ## Training the model\n",
    "    device = torch.device(cuda_name if torch.cuda.is_available() else \"cpu\")\n",
    "    # print(device)\n",
    "    model = modeling(dropout=DROPOUT, hid_dim = HID_DIM, heads = HEADER)\n",
    "    model.to(device)\n",
    "    Loss = nn.CrossEntropyLoss()\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    optimizer = RAdam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    best_auc,best_epoch = 0, -1\n",
    "    file_AUCs = \"./logs/train_\" + '_'.join(map(str, Hyper)) + '.logs'\n",
    "    \n",
    "    AUC = ('Epoch\\tTime(sec)\\tLoss_train\\tAUC_train\\tPRC_train\\tAUC_dev\\tPRC_dev\\tAUC_test\\tPRC_test')\n",
    "    with open(file_AUCs, 'w') as f:\n",
    "        f.write(AUC + '\\n')\n",
    "    model_file_name = 'model_' + '_'.join(map(str, Hyper))+  '.model'\n",
    "    # result_file_name = 'result_' + model_st + '_' + dataset +  '.csv'\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        loss = train(model, device, train_loader,Loss ,optimizer, epoch+1)\n",
    "        AUC_train,PRC_train,_,_,_ = predicting(model,device, Loss,train_loader)\n",
    "        AUC_dev,PRC_dev,LOSS_dev,_,_ = predicting(model,device, Loss,valid_loader)\n",
    "        AUC_test, PRC_test,_,_,_ = predicting(model,device, Loss,test_loader)\n",
    "        AUC_vertex, PRC_vertex,_,_,_ = predicting(model,device, Loss,vertex_loader)\n",
    "        _,_,_,AUC_barelier,PRC_barelier = predicting(model,device,Loss,barelier_loader)\n",
    "        end = timeit.default_timer()\n",
    "        time = end - start\n",
    "        AUCs = [epoch,time,loss,AUC_train,PRC_train,AUC_dev,PRC_dev,AUC_test,PRC_test,AUC_vertex, PRC_vertex,AUC_barelier, PRC_barelier,LOSS_dev]\n",
    "        # print('\\t'.join(map(str, [round(i,4) for i in AUCs])))\n",
    "        save_AUCs(AUCs,file_AUCs)\n",
    "        if AUC_dev > max_auc:\n",
    "            torch.save(model.state_dict(), \"./logs/\" + model_file_name)\n",
    "            max_auc = AUC_dev\n",
    "            re_AUCs = AUCs\n",
    "        # Early Stop\n",
    "        epoch_valid_value = AUC_dev\n",
    "        if epoch_valid_value > best_epoch_value:\n",
    "            best_epoch_value = epoch_valid_value\n",
    "            early_stop_count = 0\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "        if early_stop_count >= 15:\n",
    "            break\n",
    "    result_AUCs = './logs/result_logs.txt'\n",
    "    save_AUCs(['_'.join(map(str, Hyper))] + [round(i,4) for i in re_AUCs],result_AUCs)\n",
    "    return re_AUCs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size:  1024\n",
      "Learning rate:  0.001\n",
      "Dropout:  0.1\n",
      "Hidden dimension:  512\n",
      "Header:  16\n",
      "Number of epoch:  401\n",
      "Weight decay:  0.0001\n",
      "Cuda name: cuda:0\n",
      "Pre-processed data found: /home/tbyang/Desktop/GraphBSM3/data/Datasets/processed/BSMset_DelVerBare_train.pt, loading ...\n",
      "Pre-processed data found: /home/tbyang/Desktop/GraphBSM3/data/Datasets/processed/BSMset_DelVerBare_train_t.pt, loading ...\n",
      "Pre-processed data found: /home/tbyang/Desktop/GraphBSM3/data/Datasets/processed/BSMset_DelVerBare_valid.pt, loading ...\n",
      "Pre-processed data found: /home/tbyang/Desktop/GraphBSM3/data/Datasets/processed/BSMset_DelVerBare_valid_t.pt, loading ...\n",
      "Pre-processed data found: /home/tbyang/Desktop/GraphBSM3/data/Datasets/processed/BSMset_DelVerBare_test.pt, loading ...\n",
      "Pre-processed data found: /home/tbyang/Desktop/GraphBSM3/data/Datasets/processed/BSMset_DelVerBare_test_t.pt, loading ...\n",
      "Pre-processed data found: /home/tbyang/Desktop/GraphBSM3/data/Datasets/processed/Vertex.pt, loading ...\n",
      "Pre-processed data found: /home/tbyang/Desktop/GraphBSM3/data/Datasets/processed/Vertex_t.pt, loading ...\n",
      "Pre-processed data found: /home/tbyang/Desktop/GraphBSM3/data/Datasets/processed/Barelier.pt, loading ...\n",
      "Pre-processed data found: /home/tbyang/Desktop/GraphBSM3/data/Datasets/processed/Barelier_t.pt, loading ...\n",
      "running on  GATNet_BSMset_DelVerBare_train\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    start = timeit.default_timer()\n",
    "    \n",
    "    ## Hyper-parameter\n",
    "    BATCH_SIZE = 1024;      print('Batch size: ', BATCH_SIZE)\n",
    "    LR = 0.001;             print('Learning rate: ', LR)\n",
    "    DROPOUT = 0.1;          print('Dropout: ', DROPOUT)\n",
    "    HID_DIM = 512;          print('Hidden dimension: ', HID_DIM)\n",
    "    HEADER = 16;            print('Header: ', HEADER)\n",
    "    NUM_EPOCHS = 401;       print('Number of epoch: ', NUM_EPOCHS)\n",
    "    WEIGHT_DECAY = 0.0001;  print('Weight decay: ', WEIGHT_DECAY)\n",
    "    CUDA_NAME = \"cuda:0\";   print('Cuda name:', CUDA_NAME)\n",
    "    \n",
    "    modeling = GATNet\n",
    "    hypers = modeling,BATCH_SIZE,LR,WEIGHT_DECAY,NUM_EPOCHS,DROPOUT,HID_DIM,SEED,HEADER,CUDA_NAME\n",
    "    re_AUCs = training_gat(hypers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
